{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ceb43e",
   "metadata": {},
   "source": [
    "# Cross‑Analysis Recap — Notebook\n",
    "\n",
    "This notebook scans a **results root folder** containing multiple experiment subfolders (e.g. `univ_delta_barthel`, `univ_theta_barthel`, `univ_exponent_barthel`, … — and the corresponding ones for FIM and Effectiveness).  \n",
    "It aggregates the CSVs produced by your per‑experiment analysis (e.g. `metric_scores.csv`, `fold_scores.csv`, `feature_importances_mean.csv`, `predictions.csv`, `fold_predictions_long.csv`), compares **feature families** (periodic / aperiodic / complexity) across targets, tries to infer **ROIs/hemisphere** patterns, and generates a concise **Markdown report** + **plots** suitable for your supervisor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc57d6",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. In the **Config** cell below, set `results_root` to your directory that contains all the sub‑analyses.  \n",
    "2. Run the notebook top-to-bottom.  \n",
    "3. Outputs (plots/CSVs/report) will be saved under `outdir` and previewed inline where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c26642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e557221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def read_csv_rows(path: Path) -> List[Dict[str, str]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        return [row for row in r]\n",
    "\n",
    "def write_csv_rows(path: Path, rows: List[Dict[str, Any]], fieldnames: Optional[List[str]] = None):\n",
    "    if not rows:\n",
    "        return\n",
    "    if fieldnames is None:\n",
    "        fieldnames = list(rows[0].keys())\n",
    "        extra = set().union(*[set(r.keys()) for r in rows]) - set(fieldnames)\n",
    "        fieldnames += sorted(extra)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def best_metric_from_rows(metric_rows: List[Dict[str,str]]) -> Tuple[str, float, str]:\n",
    "    \"\"\"\n",
    "    Choose the primary metric and value for comparison.\n",
    "    Priority: R2 (max) -> RMSE (min) -> MAE (min) -> Accuracy (max) -> F1 (max).\n",
    "    Returns (metric_name, score_value, direction) where direction in {\"higher_is_better\",\"lower_is_better\"}.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for r in metric_rows:\n",
    "        k = (r.get(\"metric\") or \"\").lower()\n",
    "        v = safe_float(r.get(\"value\", np.nan))\n",
    "        if k:\n",
    "            m[k] = v\n",
    "    candidates = []\n",
    "    if \"r2\" in m:\n",
    "        candidates.append((\"r2\", m[\"r2\"], \"higher_is_better\"))\n",
    "    if \"rmse\" in m:\n",
    "        candidates.append((\"rmse\", m[\"rmse\"], \"lower_is_better\"))\n",
    "    if \"mae\" in m:\n",
    "        candidates.append((\"mae\", m[\"mae\"], \"lower_is_better\"))\n",
    "    if \"accuracy\" in m:\n",
    "        candidates.append((\"accuracy\", m[\"accuracy\"], \"higher_is_better\"))\n",
    "    if \"f1\" in m:\n",
    "        candidates.append((\"f1\", m[\"f1\"], \"higher_is_better\"))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    return (\"\", np.nan, \"higher_is_better\")\n",
    "\n",
    "def infer_target_from_path(p: Path) -> str:\n",
    "    s = p.as_posix().lower()\n",
    "    if \"barthel\" in s:\n",
    "        return \"barthel\"\n",
    "    if \"fim\" in s:\n",
    "        return \"fim\"\n",
    "    if \"effectiveness\" in s:\n",
    "        return \"effectiveness\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_family_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    if any(k in s for k in [\"delta\",\"theta\"]):\n",
    "        return \"periodic\"\n",
    "    if any(k in s for k in [\"exponent\",\"offset\"]):\n",
    "        return \"aperiodic\"\n",
    "    if any(k in s for k in [\"lziv\",\"higuci\",\"higuchi\"]):\n",
    "        return \"complexity\"\n",
    "    return \"other\"\n",
    "\n",
    "def infer_feature_key_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    for k in [\"delta\",\"theta\",\"exponent\",\"offset\",\"lziv\",\"higuci\",\"higuchi\"]:\n",
    "        if k in s:\n",
    "            return k\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_roi_from_importances(imp_rows: List[Dict[str,str]]) -> Optional[str]:\n",
    "    if not imp_rows:\n",
    "        return None\n",
    "    feats = [r.get(\"feature\",\"\") for r in imp_rows if r.get(\"feature\")]\n",
    "    text = \"_\".join(feats).lower()\n",
    "    rois = [\"frontal\",\"temporal\",\"parietal\",\"occipital\",\"central\",\"cingulate\",\"hippocampus\",\"insula\"]\n",
    "    sides = [\"left\",\"right\",\"l\",\"r\"]\n",
    "    found_roi = None\n",
    "    for roi in rois:\n",
    "        if roi in text:\n",
    "            found_roi = roi\n",
    "            break\n",
    "    found_side = None\n",
    "    for s in sides:\n",
    "        import re as _re\n",
    "        if _re.search(r\"(^|[_\\-])\" + s + r\"([_\\-]|$)\", text):\n",
    "            found_side = s\n",
    "            break\n",
    "    if found_roi and found_side:\n",
    "        return f\"{found_roi}_{found_side}\"\n",
    "    if found_roi:\n",
    "        return found_roi\n",
    "    return None\n",
    "\n",
    "def infer_hemisphere(label: str) -> Optional[str]:\n",
    "    if not label:\n",
    "        return None\n",
    "    s = label.lower()\n",
    "    toks = s.split(\"_\")\n",
    "    if \"right\" in toks or \"r\" in toks:\n",
    "        return \"right\"\n",
    "    if \"left\" in toks or \"l\" in toks:\n",
    "        return \"left\"\n",
    "    return None\n",
    "\n",
    "def choose_score_for_comparison(metric_name: str, value: float, direction: str) -> float:\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    if direction == \"lower_is_better\":\n",
    "        return -value\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6b6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PATCH: supporto ai file 'metrics_by_fold.csv' & 'feature_importances_mean_std.csv' ---\n",
    "\n",
    "from pathlib import Path\n",
    "import csv, numpy as np\n",
    "\n",
    "EXPECTED_ANY = {\n",
    "    \"metric_scores.csv\",\n",
    "    \"fold_scores.csv\",\n",
    "    \"feature_importances_mean.csv\",\n",
    "    \"predictions.csv\",\n",
    "    \"fold_predictions_long.csv\",\n",
    "    # nuovi nomi visti nei tuoi run:\n",
    "    \"metrics_by_fold.csv\",\n",
    "    \"feature_importances_mean_std.csv\",\n",
    "    \"feature_importances_by_fold.csv\",\n",
    "}\n",
    "\n",
    "def _read_csv_rows(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def _coerce_metric_rows_from_metrics_by_fold(exp_dir: Path):\n",
    "    \"\"\"\n",
    "    Converte 'metrics_by_fold.csv' in una lista tipo metric_scores.csv:\n",
    "      [{'metric':'r2','value':...}, {'metric':'rmse','value':...}, ...]\n",
    "    Prende la media sui fold per le colonne standard se presenti.\n",
    "    \"\"\"\n",
    "    p = exp_dir / \"metrics_by_fold.csv\"\n",
    "    rows = _read_csv_rows(p)\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    # Prova a capire quali colonne metriche sono presenti\n",
    "    # (supportiamo nomi comuni; aggiungi qui eventuali varianti tue)\n",
    "    candidates = [\"r2\",\"rmse\",\"mae\",\"mse\",\"accuracy\",\"f1\",\"mape\"]\n",
    "    metrics_present = [m for m in candidates if m in rows[0].keys()]\n",
    "    out = []\n",
    "    for m in metrics_present:\n",
    "        vals = []\n",
    "        for r in rows:\n",
    "            try:\n",
    "                vals.append(float(r[m]))\n",
    "            except Exception:\n",
    "                pass\n",
    "        if len(vals):\n",
    "            out.append({\"metric\": m, \"value\": float(np.mean(vals))})\n",
    "    return out\n",
    "\n",
    "def _coerce_fold_scores_from_metrics_by_fold(exp_dir: Path):\n",
    "    \"\"\"\n",
    "    Converte 'metrics_by_fold.csv' in un formato simile a fold_scores.csv:\n",
    "      [{'fold':0, 'r2':..., 'rmse':...}, ...]\n",
    "    Se non c'è una colonna fold, genera un indice.\n",
    "    \"\"\"\n",
    "    p = exp_dir / \"metrics_by_fold.csv\"\n",
    "    rows = _read_csv_rows(p)\n",
    "    if not rows:\n",
    "        return []\n",
    "    # prova a usare 'fold' se esiste, altrimenti enumerazione\n",
    "    has_fold = \"fold\" in rows[0]\n",
    "    out = []\n",
    "    for i, r in enumerate(rows):\n",
    "        row = {\"fold\": int(r[\"fold\"]) if has_fold and r.get(\"fold\",\"\").isdigit() else i}\n",
    "        for k, v in r.items():\n",
    "            if k.lower() == \"fold\":\n",
    "                continue\n",
    "            try:\n",
    "                row[k.lower()] = float(v)\n",
    "            except Exception:\n",
    "                pass\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "def _coerce_imp_rows_from_mean_std(exp_dir: Path):\n",
    "    \"\"\"\n",
    "    Converte 'feature_importances_mean_std.csv' in formato:\n",
    "      [{'feature':..., 'mean_importance':..., 'std_importance':...}, ...]\n",
    "    Supporta colonne: feature, (mean|mean_importance), (std|std_importance).\n",
    "    \"\"\"\n",
    "    p = exp_dir / \"feature_importances_mean_std.csv\"\n",
    "    rows = _read_csv_rows(p)\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    # Riconosci nomi colonne flessibili\n",
    "    def pick(colnames, keys):\n",
    "        keys_low = {k.lower(): k for k in keys}\n",
    "        for c in colnames:\n",
    "            if c in keys_low:\n",
    "                return keys_low[c]\n",
    "        return None\n",
    "\n",
    "    keys = rows[0].keys()\n",
    "    c_feat = pick([\"feature\",\"features\",\"name\"], keys)\n",
    "    c_mean = pick([\"mean_importance\",\"mean\",\"avg\"], keys)\n",
    "    c_std  = pick([\"std_importance\",\"std\",\"sd\"], keys)\n",
    "\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        feat = r.get(c_feat, \"\") if c_feat else \"\"\n",
    "        mi = r.get(c_mean, \"\")\n",
    "        sd = r.get(c_std, \"\")\n",
    "        try:\n",
    "            mi = float(mi)\n",
    "        except Exception:\n",
    "            mi = np.nan\n",
    "        try:\n",
    "            sd = float(sd)\n",
    "        except Exception:\n",
    "            sd = np.nan\n",
    "        if feat:\n",
    "            out.append({\"feature\": feat, \"mean_importance\": mi, \"std_importance\": sd})\n",
    "    return out\n",
    "\n",
    "def summarize_experiment(exp_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Versione robusta che usa:\n",
    "      - metric_scores.csv se presente, altrimenti metrics_by_fold.csv\n",
    "      - fold_scores.csv se presente, altrimenti metrics_by_fold.csv\n",
    "      - feature_importances_mean.csv se presente, altrimenti feature_importances_mean_std.csv\n",
    "    \"\"\"\n",
    "    # --- metriche ---\n",
    "    metric_rows = _read_csv_rows(exp_dir / \"metric_scores.csv\")\n",
    "    if not metric_rows:\n",
    "        metric_rows = _coerce_metric_rows_from_metrics_by_fold(exp_dir)\n",
    "\n",
    "    # --- fold scores ---\n",
    "    fold_rows = _read_csv_rows(exp_dir / \"fold_scores.csv\")\n",
    "    if not fold_rows:\n",
    "        fold_rows = _coerce_fold_scores_from_metrics_by_fold(exp_dir)\n",
    "\n",
    "    # --- importances ---\n",
    "    imp_rows = _read_csv_rows(exp_dir / \"feature_importances_mean.csv\")\n",
    "    if not imp_rows:\n",
    "        imp_rows = _coerce_imp_rows_from_mean_std(exp_dir)\n",
    "\n",
    "    # (Questi due spesso non ci sono nelle tue cartelle ROI)\n",
    "    pred_rows  = _read_csv_rows(exp_dir / \"predictions.csv\")\n",
    "    fold_long  = _read_csv_rows(exp_dir / \"fold_predictions_long.csv\")\n",
    "\n",
    "    # --- inferenze standard del notebook ---\n",
    "    target = infer_target_from_path(exp_dir)\n",
    "    family = infer_family_from_path(exp_dir)\n",
    "    feature_key = infer_feature_key_from_path(exp_dir)\n",
    "    roi_guess = infer_roi_from_importances(imp_rows) if imp_rows else None\n",
    "\n",
    "    # Scegli metrica primaria\n",
    "    metric_name, metric_value, direction = best_metric_from_rows(metric_rows)\n",
    "\n",
    "    # Fallback R2 dai fold_long se non abbiamo metriche\n",
    "    if not metric_name and fold_long:\n",
    "        by_fold = {}\n",
    "        for r in fold_long:\n",
    "            f = int(r.get(\"fold\", -1))\n",
    "            by_fold.setdefault(f, {\"y_true\": [], \"y_pred\": []})\n",
    "            try:\n",
    "                by_fold[f][\"y_true\"].append(float(r.get(\"y_true\", np.nan)))\n",
    "                by_fold[f][\"y_pred\"].append(float(r.get(\"y_pred\", np.nan)))\n",
    "            except Exception:\n",
    "                pass\n",
    "        r2s = []\n",
    "        for f, data in by_fold.items():\n",
    "            y_t = np.asarray(data[\"y_true\"], float)\n",
    "            y_p = np.asarray(data[\"y_pred\"], float)\n",
    "            ss_res = np.nansum((y_t - y_p)**2)\n",
    "            ss_tot = np.nansum((y_t - np.nanmean(y_t))**2)\n",
    "            r2 = np.nan if ss_tot==0 else 1.0 - ss_res/ss_tot\n",
    "            r2s.append(r2)\n",
    "        if r2s:\n",
    "            metric_name = \"r2\"\n",
    "            metric_value = float(np.nanmean(r2s))\n",
    "            direction = \"higher_is_better\"\n",
    "\n",
    "    # Stima n per fold se possibile\n",
    "    n_per_fold = 0\n",
    "    if fold_rows and \"fold\" in fold_rows[0]:\n",
    "        counts = {}\n",
    "        for r in fold_rows:\n",
    "            f = int(r.get(\"fold\", -1))\n",
    "            counts[f] = counts.get(f, 0) + 1\n",
    "        if counts:\n",
    "            n_per_fold = int(np.mean(list(counts.values())))\n",
    "\n",
    "    # Top feature da importances\n",
    "    top_feature = \"\"\n",
    "    top_mean_imp = np.nan\n",
    "    top_std_imp = np.nan\n",
    "    if imp_rows:\n",
    "        rows_sorted = sorted(imp_rows, key=lambda r: float(r.get(\"mean_importance\", \"nan\")), reverse=True)\n",
    "        if rows_sorted:\n",
    "            top_feature = rows_sorted[0].get(\"feature\",\"\")\n",
    "            top_mean_imp = float(rows_sorted[0].get(\"mean_importance\", \"nan\") or np.nan)\n",
    "            top_std_imp = float(rows_sorted[0].get(\"std_importance\", \"nan\") or np.nan)\n",
    "\n",
    "    return {\n",
    "        \"exp_dir\": exp_dir.as_posix(),\n",
    "        \"target\": target,\n",
    "        \"family\": family,\n",
    "        \"feature_key\": feature_key,\n",
    "        \"roi_guess\": roi_guess or \"\",\n",
    "        \"metric_name\": metric_name,\n",
    "        \"metric_value\": metric_value,\n",
    "        \"metric_direction\": direction,\n",
    "        \"score_for_compare\": choose_score_for_comparison(metric_name, metric_value, direction) if metric_name else np.nan,\n",
    "        \"n_per_fold_test\": n_per_fold,\n",
    "        \"top_feature\": top_feature,\n",
    "        \"top_feature_mean_imp\": top_mean_imp,\n",
    "        \"top_feature_std_imp\": top_std_imp,\n",
    "    }\n",
    "\n",
    "def scan_results_root(root: Path) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Tollerante: considera 'esperimento' qualsiasi cartella che contenga\n",
    "    ALMENO uno tra i file attesi (vecchi o nuovi).\n",
    "    \"\"\"\n",
    "    experiments = set()\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        if p.name in EXPECTED_ANY or p.name.startswith(\"best_fold_\") and p.name.endswith(\"_feature_importances.csv\"):\n",
    "            experiments.add(p.parent)\n",
    "    summaries = [summarize_experiment(p) for p in sorted(experiments)]\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7fcc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results root: /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\n",
      "Output dir  : /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/_recap\n"
     ]
    }
   ],
   "source": [
    "results_root = Path(\"/Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\")\n",
    "outdir = results_root / \"_recap\"\n",
    "\n",
    "print(\"Results root:\", results_root)\n",
    "print(\"Output dir  :\", outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f486ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "results_root = Path(\"/Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\")  # <-- conferma questo path\n",
    "\n",
    "EXPECTED_ANY = {\n",
    "    \"metric_scores.csv\",\n",
    "    \"fold_scores.csv\",\n",
    "    \"feature_importances_mean.csv\",\n",
    "    \"predictions.csv\",\n",
    "    \"fold_predictions_long.csv\",\n",
    "    # i tuoi nomi reali:\n",
    "    \"metrics_by_fold.csv\",\n",
    "    \"feature_importances_mean_std.csv\",\n",
    "    \"feature_importances_by_fold.csv\",\n",
    "}\n",
    "\n",
    "csvs = list(results_root.rglob(\"*.csv\"))\n",
    "print(f\"CSV totali trovati: {len(csvs)}\")\n",
    "for p in csvs[:15]:\n",
    "    print(\"-\", p)\n",
    "\n",
    "# Cartelle candidate esperimento: contengono almeno UN file tra quelli attesi\n",
    "experiments = set()\n",
    "for p in csvs:\n",
    "    name = p.name.lower()\n",
    "    if (name in {n.lower() for n in EXPECTED_ANY}) or (name.startswith(\"best_fold_\") and name.endswith(\"_feature_importances.csv\")):\n",
    "        experiments.add(p.parent)\n",
    "\n",
    "experiments = sorted(experiments)\n",
    "print(f\"\\nCartelle esperimento trovate: {len(experiments)}\")\n",
    "for d in experiments[:20]:\n",
    "    print(\"-\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23928e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 experiment folders with CSVs.\n",
      "Report written to: /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/_recap/cross_analysis_recap.md\n"
     ]
    }
   ],
   "source": [
    "summaries = scan_results_root(results_root)\n",
    "print(f\"Found {len(summaries)} experiment folders with CSVs.\")\n",
    "\n",
    "report_path = write_report(summaries, outdir)\n",
    "print(\"Report written to:\", report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee9bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 CSV files under /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_effectiveness/metrics_by_fold.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_effectiveness/best_fold_2_feature_importances.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_effectiveness/feature_importances_by_fold.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_effectiveness/feature_importances_mean_std.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_barthel/metrics_by_fold.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_barthel/best_fold_5_feature_importances.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_barthel/feature_importances_by_fold.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_offset_barthel/feature_importances_mean_std.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_theta_fim/metrics_by_fold.csv\n",
      "- /Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI/univ_theta_fim/feature_importances_by_fold.csv\n",
      "\n",
      "Experiment-like folders found: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1) Count and preview all CSVs under results_root\n",
    "csvs = list(results_root.rglob(\"*.csv\"))\n",
    "print(f\"Found {len(csvs)} CSV files under {results_root}\")\n",
    "for p in csvs[:10]:\n",
    "    print(\"-\", p)\n",
    "\n",
    "# 2) Show which folders contain at least one of the expected CSV names\n",
    "expected = {\"metric_scores.csv\",\"fold_scores.csv\",\"feature_importances_mean.csv\",\n",
    "            \"predictions.csv\",\"fold_predictions_long.csv\"}\n",
    "exp_dirs = set()\n",
    "for p in csvs:\n",
    "    if p.name in expected:\n",
    "        exp_dirs.add(p.parent)\n",
    "print(f\"\\nExperiment-like folders found: {len(exp_dirs)}\")\n",
    "for d in list(sorted(exp_dirs))[:10]:\n",
    "    print(\"-\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "485fb888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report not found yet.\n"
     ]
    }
   ],
   "source": [
    "rep = (outdir / \"cross_analysis_recap.md\")\n",
    "if rep.exists():\n",
    "    with open(rep, \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"Report not found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "outdir = results_root / \"_recap\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_csv_rows(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def safe_float(x, default=np.nan):\n",
    "    try: return float(x)\n",
    "    except: return default\n",
    "\n",
    "def infer_target_from_path(p: Path) -> str:\n",
    "    s = p.as_posix().lower()\n",
    "    if \"barthel\" in s: return \"barthel\"\n",
    "    if \"fim\" in s: return \"fim\"\n",
    "    if \"effectiveness\" in s: return \"effectiveness\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_family_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    if any(k in s for k in [\"delta\",\"theta\"]): return \"periodic\"\n",
    "    if any(k in s for k in [\"exponent\",\"offset\"]): return \"aperiodic\"\n",
    "    if any(k in s for k in [\"lziv\",\"higuci\",\"higuchi\"]): return \"complexity\"\n",
    "    return \"other\"\n",
    "\n",
    "def infer_feature_key_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    for k in [\"delta\",\"theta\",\"exponent\",\"offset\",\"lziv\",\"higuci\",\"higuchi\"]:\n",
    "        if k in s: return k\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_roi_from_importances(imp_rows):\n",
    "    if not imp_rows: return \"\"\n",
    "    feats = [r.get(\"feature\",\"\") for r in imp_rows if r.get(\"feature\")]\n",
    "    text = \"_\".join(feats).lower()\n",
    "    rois = [\"frontal\",\"temporal\",\"parietal\",\"occipital\",\"central\",\"cingulate\",\"hippocampus\",\"insula\"]\n",
    "    sides = [\"left\",\"right\",\"l\",\"r\"]\n",
    "    found_roi = next((roi for roi in rois if roi in text), None)\n",
    "    found_side = None\n",
    "    for s in sides:\n",
    "        if re.search(rf\"(^|[_\\-]){s}([_\\-]|$)\", text):\n",
    "            found_side = s; break\n",
    "    if found_roi and found_side: return f\"{found_roi}_{found_side}\"\n",
    "    return found_roi or \"\"\n",
    "\n",
    "def choose_score_for_comparison(metric_name, value, direction):\n",
    "    if value is None or np.isnan(value): return np.nan\n",
    "    if direction == \"lower_is_better\": return -value\n",
    "    return value\n",
    "\n",
    "def coerce_metric_rows_from_metrics_by_fold(exp_dir: Path):\n",
    "    rows = read_csv_rows(exp_dir / \"metrics_by_fold.csv\")\n",
    "    if not rows: return []\n",
    "    # cerca colonne comuni (case-insensitive)\n",
    "    keys = {k.lower(): k for k in rows[0].keys()}\n",
    "    candidates = [\"r2\",\"rmse\",\"mae\",\"mse\",\"accuracy\",\"f1\",\"mape\"]\n",
    "    out = []\n",
    "    for m in candidates:\n",
    "        k = keys.get(m)\n",
    "        if not k: continue\n",
    "        vals = [safe_float(r.get(k)) for r in rows]\n",
    "        vals = [v for v in vals if np.isfinite(v)]\n",
    "        if vals:\n",
    "            out.append({\"metric\": m, \"value\": float(np.mean(vals))})\n",
    "    return out\n",
    "\n",
    "def coerce_imp_rows_from_mean_std(exp_dir: Path):\n",
    "    rows = read_csv_rows(exp_dir / \"feature_importances_mean_std.csv\")\n",
    "    if not rows: return []\n",
    "    keys = {k.lower(): k for k in rows[0].keys()}\n",
    "    c_feat = keys.get(\"feature\") or keys.get(\"features\") or keys.get(\"name\")\n",
    "    c_mean = keys.get(\"mean_importance\") or keys.get(\"mean\") or keys.get(\"avg\")\n",
    "    c_std  = keys.get(\"std_importance\")  or keys.get(\"std\")  or keys.get(\"sd\")\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        feat = r.get(c_feat, \"\") if c_feat else \"\"\n",
    "        mi = safe_float(r.get(c_mean)) if c_mean else np.nan\n",
    "        sd = safe_float(r.get(c_std)) if c_std else np.nan\n",
    "        if feat:\n",
    "            out.append({\"feature\": feat, \"mean_importance\": mi, \"std_importance\": sd})\n",
    "    return out\n",
    "\n",
    "def best_metric_from_rows(metric_rows):\n",
    "    # priorità: R2 -> RMSE -> MAE -> Accuracy -> F1\n",
    "    m = { (r.get(\"metric\") or \"\").lower(): safe_float(r.get(\"value\")) for r in metric_rows }\n",
    "    if \"r2\" in m:      return (\"r2\", m[\"r2\"], \"higher_is_better\")\n",
    "    if \"rmse\" in m:    return (\"rmse\", m[\"rmse\"], \"lower_is_better\")\n",
    "    if \"mae\" in m:     return (\"mae\", m[\"mae\"], \"lower_is_better\")\n",
    "    if \"accuracy\" in m:return (\"accuracy\", m[\"accuracy\"], \"higher_is_better\")\n",
    "    if \"f1\" in m:      return (\"f1\", m[\"f1\"], \"higher_is_better\")\n",
    "    return (\"\", np.nan, \"higher_is_better\")\n",
    "\n",
    "# 1) individua esperimenti (tollerante, usa i tuoi nomi)\n",
    "EXPECTED_ANY = {\"metrics_by_fold.csv\",\"feature_importances_mean_std.csv\",\"feature_importances_by_fold.csv\",\n",
    "                \"metric_scores.csv\",\"fold_scores.csv\",\"feature_importances_mean.csv\",\"predictions.csv\",\"fold_predictions_long.csv\"}\n",
    "\n",
    "experiments = sorted({\n",
    "    p.parent for p in results_root.rglob(\"*.csv\")\n",
    "    if (p.name.lower() in {n.lower() for n in EXPECTED_ANY}) or (p.name.lower().startswith(\"best_fold_\") and p.name.lower().endswith(\"_feature_importances.csv\"))\n",
    "})\n",
    "\n",
    "print(\"Esperimenti trovati:\", len(experiments))\n",
    "for d in experiments[:10]:\n",
    "    print(\"-\", d)\n",
    "\n",
    "# 2) costruisci il summary per ciascun esperimento\n",
    "summaries = []\n",
    "for exp in experiments:\n",
    "    metric_rows = coerce_metric_rows_from_metrics_by_fold(exp)\n",
    "    imp_rows = coerce_imp_rows_from_mean_std(exp)\n",
    "\n",
    "    target = infer_target_from_path(exp)\n",
    "    family = infer_family_from_path(exp)\n",
    "    feature_key = infer_feature_key_from_path(exp)\n",
    "    roi_guess = infer_roi_from_importances(imp_rows)\n",
    "\n",
    "    metric_name, metric_value, direction = best_metric_from_rows(metric_rows)\n",
    "    score_for_compare = choose_score_for_comparison(metric_name, metric_value, direction) if metric_name else np.nan\n",
    "\n",
    "    # Top feature\n",
    "    top_feature = \"\"; top_mean_imp = np.nan; top_std_imp = np.nan\n",
    "    if imp_rows:\n",
    "        imp_sorted = sorted(imp_rows, key=lambda r: r[\"mean_importance\"] if np.isfinite(r[\"mean_importance\"]) else -1, reverse=True)\n",
    "        if imp_sorted:\n",
    "            top_feature = imp_sorted[0][\"feature\"]\n",
    "            top_mean_imp = imp_sorted[0][\"mean_importance\"]\n",
    "            top_std_imp = imp_sorted[0][\"std_importance\"]\n",
    "\n",
    "    summaries.append({\n",
    "        \"exp_dir\": exp.as_posix(),\n",
    "        \"target\": target,\n",
    "        \"family\": family,\n",
    "        \"feature_key\": feature_key,\n",
    "        \"roi_guess\": roi_guess or \"\",\n",
    "        \"metric_name\": metric_name,\n",
    "        \"metric_value\": metric_value,\n",
    "        \"metric_direction\": direction,\n",
    "        \"score_for_compare\": score_for_compare,\n",
    "        \"top_feature\": top_feature,\n",
    "        \"top_feature_mean_imp\": top_mean_imp,\n",
    "        \"top_feature_std_imp\": top_std_imp,\n",
    "    })\n",
    "\n",
    "# 3) salva tabella\n",
    "def write_csv_rows(path: Path, rows, fieldnames=None):\n",
    "    if not rows: return\n",
    "    if fieldnames is None:\n",
    "        fieldnames = list(rows[0].keys())\n",
    "        extra = set().union(*[set(r.keys()) for r in rows]) - set(fieldnames)\n",
    "        fieldnames += sorted(extra)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "write_csv_rows(outdir / \"experiments_summary.csv\", summaries)\n",
    "print(\"Salvato:\", outdir / \"experiments_summary.csv\")\n",
    "\n",
    "# 4) grafici per target\n",
    "def plot_best_by_family(summaries, target, outdir):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows: \n",
    "        print(\"[skip] best_by_family\", target); \n",
    "        return\n",
    "    best = {}\n",
    "    for r in rows:\n",
    "        fam = r[\"family\"]\n",
    "        if fam not in best or r[\"score_for_compare\"] > best[fam][\"score_for_compare\"]:\n",
    "            best[fam] = r\n",
    "    labels = list(best.keys())\n",
    "    vals = [best[k][\"score_for_compare\"] for k in labels]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Best by family — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"best_by_family_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_rois(summaries, target, outdir, top_k=12):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows: \n",
    "        print(\"[skip] top_rois\", target); \n",
    "        return\n",
    "    best_by_roi = {}\n",
    "    for r in rows:\n",
    "        roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "        if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "            best_by_roi[roi] = r\n",
    "    items = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:top_k]\n",
    "    labels = [k for k,_ in items]\n",
    "    vals = [v[\"score_for_compare\"] for _, v in items]\n",
    "    plt.figure(figsize=(10, max(4, 0.5*len(labels))))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Top ROIs — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"top_rois_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def hemisphere_from_label(label: str):\n",
    "    s = (label or \"\").lower().split(\"_\")\n",
    "    if \"right\" in s or \"r\" in s: return \"right\"\n",
    "    if \"left\" in s or \"l\" in s: return \"left\"\n",
    "    return None\n",
    "\n",
    "def plot_hemisphere_summary(summaries, target, outdir):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows:\n",
    "        print(\"[skip] hemisphere\", target)\n",
    "        return\n",
    "    left = []; right = []\n",
    "    for r in rows:\n",
    "        hemi = hemisphere_from_label(r.get(\"roi_guess\",\"\"))\n",
    "        if hemi == \"left\": left.append(r[\"score_for_compare\"])\n",
    "        elif hemi == \"right\": right.append(r[\"score_for_compare\"])\n",
    "    labels = [\"left\",\"right\"]\n",
    "    vals = [float(np.nanmean(left)) if left else 0.0,\n",
    "            float(np.nanmean(right)) if right else 0.0]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"Mean comparable score\")\n",
    "    plt.title(f\"Hemisphere summary — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"hemisphere_summary_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "targets = sorted(set(s[\"target\"] for s in summaries))\n",
    "for t in targets:\n",
    "    plot_best_by_family(summaries, t, outdir)\n",
    "    plot_top_rois(summaries, t, outdir)\n",
    "    plot_hemisphere_summary(summaries, t, outdir)\n",
    "\n",
    "# 5) report markdown\n",
    "md = []\n",
    "md.append(\"# Cross-analysis Recap\\n\")\n",
    "md.append(\"Confronto tra **periodic (delta/theta)**, **aperiodic (exponent/offset)** e **complexity (lziv/higuci)** sui target (Barthel, FIM, Effectiveness).\")\n",
    "md.append(\"Gli score sono resi comparabili come *higher is better* (MAE/RMSE negati).\\\\n\")\n",
    "\n",
    "for t in targets:\n",
    "    rows_t = [s for s in summaries if s[\"target\"]==t and np.isfinite(s[\"score_for_compare\"])]\n",
    "    md.append(f\"## Target: {t}\\\\n\")\n",
    "    if not rows_t:\n",
    "        md.append(\"_Nessun esperimento valido._\\\\n\")\n",
    "        continue\n",
    "    by_fam = {}\n",
    "    for r in rows_t:\n",
    "        fam = r[\"family\"]\n",
    "        if fam not in by_fam or r[\"score_for_compare\"] > by_fam[fam][\"score_for_compare\"]:\n",
    "            by_fam[fam] = r\n",
    "    md.append(\"**Best by family**:\\\\n\")\n",
    "    for fam, r in by_fam.items():\n",
    "        md.append(f\"- {fam}: `{r['metric_name']}={r['metric_value']:.4f}` @ ROI `{r['roi_guess'] or r['feature_key']}` (dir={r['metric_direction']})\")\n",
    "    md.append(\"\")\n",
    "    best_by_roi = {}\n",
    "    for r in rows_t:\n",
    "        roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "        if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "            best_by_roi[roi] = r\n",
    "    top5 = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:5]\n",
    "    md.append(\"**Top ROIs overall**:\\\\n\")\n",
    "    for i,(roi, rr) in enumerate(top5, 1):\n",
    "        md.append(f\"{i}. `{roi}` — {rr['metric_name']}={rr['metric_value']:.4f} ({rr['family']})\")\n",
    "    md.append(\"\")\n",
    "    md.append(f\"![Best by family — {t}](best_by_family_{t}.png)\\\\n\")\n",
    "    md.append(f\"![Top ROIs — {t}](top_rois_{t}.png)\\\\n\")\n",
    "    md.append(f\"![Hemisphere summary — {t}](hemisphere_summary_{t}.png)\\\\n\")\n",
    "\n",
    "report_path = outdir / \"cross_analysis_recap.md\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(md))\n",
    "\n",
    "print(\"Report scritto in:\", report_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
