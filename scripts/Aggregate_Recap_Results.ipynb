{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ceb43e",
   "metadata": {},
   "source": [
    "# Cross‑Analysis Recap — Notebook\n",
    "\n",
    "This notebook scans a **results root folder** containing multiple experiment subfolders (e.g. `univ_delta_barthel`, `univ_theta_barthel`, `univ_exponent_barthel`, … — and the corresponding ones for FIM and Effectiveness).  \n",
    "It aggregates the CSVs produced by your per‑experiment analysis (e.g. `metric_scores.csv`, `fold_scores.csv`, `feature_importances_mean.csv`, `predictions.csv`, `fold_predictions_long.csv`), compares **feature families** (periodic / aperiodic / complexity) across targets, tries to infer **ROIs/hemisphere** patterns, and generates a concise **Markdown report** + **plots** suitable for your supervisor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc57d6",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. In the **Config** cell below, set `results_root` to your directory that contains all the sub‑analyses.  \n",
    "2. Run the notebook top-to-bottom.  \n",
    "3. Outputs (plots/CSVs/report) will be saved under `outdir` and previewed inline where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c26642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, csv, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def read_csv_rows(path: Path) -> List[Dict[str, str]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        r = csv.DictReader(f)\n",
    "        return [row for row in r]\n",
    "\n",
    "def write_csv_rows(path: Path, rows: List[Dict[str, Any]], fieldnames: Optional[List[str]] = None):\n",
    "    if not rows:\n",
    "        return\n",
    "    if fieldnames is None:\n",
    "        fieldnames = list(rows[0].keys())\n",
    "        extra = set().union(*[set(r.keys()) for r in rows]) - set(fieldnames)\n",
    "        fieldnames += sorted(extra)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def safe_float(x, default=np.nan):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def best_metric_from_rows(metric_rows: List[Dict[str,str]]) -> Tuple[str, float, str]:\n",
    "    \"\"\"\n",
    "    Choose the primary metric and value for comparison.\n",
    "    Priority: R2 (max) -> RMSE (min) -> MAE (min) -> Accuracy (max) -> F1 (max).\n",
    "    Returns (metric_name, score_value, direction) where direction in {\"higher_is_better\",\"lower_is_better\"}.\n",
    "    \"\"\"\n",
    "    m = {}\n",
    "    for r in metric_rows:\n",
    "        k = (r.get(\"metric\") or \"\").lower()\n",
    "        v = safe_float(r.get(\"value\", np.nan))\n",
    "        if k:\n",
    "            m[k] = v\n",
    "    candidates = []\n",
    "    if \"r2\" in m:\n",
    "        candidates.append((\"r2\", m[\"r2\"], \"higher_is_better\"))\n",
    "    if \"rmse\" in m:\n",
    "        candidates.append((\"rmse\", m[\"rmse\"], \"lower_is_better\"))\n",
    "    if \"mae\" in m:\n",
    "        candidates.append((\"mae\", m[\"mae\"], \"lower_is_better\"))\n",
    "    if \"accuracy\" in m:\n",
    "        candidates.append((\"accuracy\", m[\"accuracy\"], \"higher_is_better\"))\n",
    "    if \"f1\" in m:\n",
    "        candidates.append((\"f1\", m[\"f1\"], \"higher_is_better\"))\n",
    "    if candidates:\n",
    "        return candidates[0]\n",
    "    return (\"\", np.nan, \"higher_is_better\")\n",
    "\n",
    "def infer_target_from_path(p: Path) -> str:\n",
    "    s = p.as_posix().lower()\n",
    "    if \"barthel\" in s:\n",
    "        return \"barthel\"\n",
    "    if \"fim\" in s:\n",
    "        return \"fim\"\n",
    "    if \"effectiveness\" in s:\n",
    "        return \"effectiveness\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_family_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    if any(k in s for k in [\"delta\",\"theta\"]):\n",
    "        return \"periodic\"\n",
    "    if any(k in s for k in [\"exponent\",\"offset\"]):\n",
    "        return \"aperiodic\"\n",
    "    if any(k in s for k in [\"lziv\",\"higuci\",\"higuchi\"]):\n",
    "        return \"complexity\"\n",
    "    return \"other\"\n",
    "\n",
    "def infer_feature_key_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    for k in [\"delta\",\"theta\",\"exponent\",\"offset\",\"lziv\",\"higuci\",\"higuchi\"]:\n",
    "        if k in s:\n",
    "            return k\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_roi_from_importances(imp_rows: List[Dict[str,str]]) -> Optional[str]:\n",
    "    if not imp_rows:\n",
    "        return None\n",
    "    feats = [r.get(\"feature\",\"\") for r in imp_rows if r.get(\"feature\")]\n",
    "    text = \"_\".join(feats).lower()\n",
    "    rois = [\"frontal\",\"temporal\",\"parietal\",\"occipital\",\"central\",\"cingulate\",\"hippocampus\",\"insula\"]\n",
    "    sides = [\"left\",\"right\",\"l\",\"r\"]\n",
    "    found_roi = None\n",
    "    for roi in rois:\n",
    "        if roi in text:\n",
    "            found_roi = roi\n",
    "            break\n",
    "    found_side = None\n",
    "    for s in sides:\n",
    "        import re as _re\n",
    "        if _re.search(r\"(^|[_\\-])\" + s + r\"([_\\-]|$)\", text):\n",
    "            found_side = s\n",
    "            break\n",
    "    if found_roi and found_side:\n",
    "        return f\"{found_roi}_{found_side}\"\n",
    "    if found_roi:\n",
    "        return found_roi\n",
    "    return None\n",
    "\n",
    "def infer_hemisphere(label: str) -> Optional[str]:\n",
    "    if not label:\n",
    "        return None\n",
    "    s = label.lower()\n",
    "    toks = s.split(\"_\")\n",
    "    if \"right\" in toks or \"r\" in toks:\n",
    "        return \"right\"\n",
    "    if \"left\" in toks or \"l\" in toks:\n",
    "        return \"left\"\n",
    "    return None\n",
    "\n",
    "def choose_score_for_comparison(metric_name: str, value: float, direction: str) -> float:\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    if direction == \"lower_is_better\":\n",
    "        return -value\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Core aggregation ----------------\n",
    "\n",
    "def summarize_experiment(exp_dir: Path) -> Dict[str, Any]:\n",
    "    metric_rows = read_csv_rows(exp_dir / \"metric_scores.csv\")\n",
    "    fold_rows   = read_csv_rows(exp_dir / \"fold_scores.csv\")\n",
    "    imp_rows    = read_csv_rows(exp_dir / \"feature_importances_mean.csv\")\n",
    "    pred_rows   = read_csv_rows(exp_dir / \"predictions.csv\")\n",
    "    fold_long   = read_csv_rows(exp_dir / \"fold_predictions_long.csv\")\n",
    "\n",
    "    target = infer_target_from_path(exp_dir)\n",
    "    family = infer_family_from_path(exp_dir)\n",
    "    feature_key = infer_feature_key_from_path(exp_dir)\n",
    "    roi_guess = infer_roi_from_importances(imp_rows) if imp_rows else None\n",
    "\n",
    "    metric_name, metric_value, direction = best_metric_from_rows(metric_rows)\n",
    "    if not metric_name and fold_long:\n",
    "        # fallback: compute mean R2 over folds\n",
    "        by_fold = {}\n",
    "        for r in fold_long:\n",
    "            f = int(r.get(\"fold\", -1))\n",
    "            by_fold.setdefault(f, {\"y_true\": [], \"y_pred\": []})\n",
    "            by_fold[f][\"y_true\"].append(safe_float(r.get(\"y_true\", np.nan)))\n",
    "            by_fold[f][\"y_pred\"].append(safe_float(r.get(\"y_pred\", np.nan)))\n",
    "        r2s = []\n",
    "        for f, data in by_fold.items():\n",
    "            y_t = np.asarray(data[\"y_true\"], float)\n",
    "            y_p = np.asarray(data[\"y_pred\"], float)\n",
    "            ss_res = np.nansum((y_t - y_p)**2)\n",
    "            ss_tot = np.nansum((y_t - np.nanmean(y_t))**2)\n",
    "            r2 = np.nan if ss_tot==0 else 1.0 - ss_res/ss_tot\n",
    "            r2s.append(r2)\n",
    "        if r2s:\n",
    "            metric_name = \"r2\"\n",
    "            metric_value = float(np.nanmean(r2s))\n",
    "            direction = \"higher_is_better\"\n",
    "\n",
    "    n_per_fold = 0\n",
    "    if fold_long:\n",
    "        by_fold = {}\n",
    "        for r in fold_long:\n",
    "            f = int(r.get(\"fold\", -1))\n",
    "            by_fold[f] = by_fold.get(f, 0) + 1\n",
    "        n_per_fold = int(np.mean(list(by_fold.values()))) if by_fold else 0\n",
    "\n",
    "    top_feature = \"\"\n",
    "    top_mean_imp = np.nan\n",
    "    top_std_imp = np.nan\n",
    "    if imp_rows:\n",
    "        rows_sorted = sorted(imp_rows, key=lambda r: safe_float(r.get(\"mean_importance\", np.nan)), reverse=True)\n",
    "        if rows_sorted:\n",
    "            top_feature = rows_sorted[0].get(\"feature\",\"\")\n",
    "            top_mean_imp = safe_float(rows_sorted[0].get(\"mean_importance\", np.nan))\n",
    "            top_std_imp = safe_float(rows_sorted[0].get(\"std_importance\", np.nan))\n",
    "\n",
    "    return {\n",
    "        \"exp_dir\": exp_dir.as_posix(),\n",
    "        \"target\": target,\n",
    "        \"family\": family,\n",
    "        \"feature_key\": feature_key,\n",
    "        \"roi_guess\": roi_guess or \"\",\n",
    "        \"metric_name\": metric_name,\n",
    "        \"metric_value\": metric_value,\n",
    "        \"metric_direction\": direction,\n",
    "        \"score_for_compare\": choose_score_for_comparison(metric_name, metric_value, direction),\n",
    "        \"n_per_fold_test\": n_per_fold,\n",
    "        \"top_feature\": top_feature,\n",
    "        \"top_feature_mean_imp\": top_mean_imp,\n",
    "        \"top_feature_std_imp\": top_std_imp,\n",
    "    }\n",
    "\n",
    "def scan_results_root(root: Path) -> List[Dict[str, Any]]:\n",
    "    experiments = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if any((p / fname).exists() for fname in [\n",
    "            \"metric_scores.csv\",\"fold_scores.csv\",\"feature_importances_mean.csv\",\n",
    "            \"predictions.csv\",\"fold_predictions_long.csv\"\n",
    "        ]):\n",
    "            experiments.append(p)\n",
    "    summaries = [summarize_experiment(p) for p in experiments]\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Plots ----------------\n",
    "\n",
    "def barplot_scores_by_family(summaries: List[Dict[str,Any]], target: str, outdir: Path):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows:\n",
    "        print(f\"[skip] best_by_family for {target}: no rows\")\n",
    "        return\n",
    "    families = {}\n",
    "    for r in rows:\n",
    "        fam = r[\"family\"]\n",
    "        if fam not in families or r[\"score_for_compare\"] > families[fam][\"score_for_compare\"]:\n",
    "            families[fam] = r\n",
    "    labels = list(families.keys())\n",
    "    vals = [families[k][\"score_for_compare\"] for k in labels]\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels, rotation=0)\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Best model per family — target: {target}\")\n",
    "    plt.tight_layout()\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(outdir / f\"best_by_family_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def barplot_top_rois(summaries: List[Dict[str,Any]], target: str, outdir: Path, top_k: int = 12):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows:\n",
    "        print(f\"[skip] top_rois for {target}: no rows\")\n",
    "        return\n",
    "    best_by_roi = {}\n",
    "    for r in rows:\n",
    "        roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "        if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "            best_by_roi[roi] = r\n",
    "    items = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:top_k]\n",
    "    labels = [k for k, _ in items]\n",
    "    vals = [v[\"score_for_compare\"] for _, v in items]\n",
    "\n",
    "    plt.figure(figsize=(10, max(4, 0.5*len(labels))))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Top ROIs (best across families) — target: {target}\")\n",
    "    plt.tight_layout()\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(outdir / f\"top_rois_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def hemisphere_balance_plot(summaries: List[Dict[str,Any]], target: str, outdir: Path):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows:\n",
    "        print(f\"[skip] hemisphere_summary for {target}: no rows\")\n",
    "        return\n",
    "    left_scores = []\n",
    "    right_scores = []\n",
    "    for r in rows:\n",
    "        hemi = infer_hemisphere(r.get(\"roi_guess\",\"\"))\n",
    "        if hemi == \"left\":\n",
    "            left_scores.append(r[\"score_for_compare\"])\n",
    "        elif hemi == \"right\":\n",
    "            right_scores.append(r[\"score_for_compare\"])\n",
    "    labels = [\"left\",\"right\"]\n",
    "    vals = [float(np.nanmean(left_scores)) if left_scores else 0.0,\n",
    "            float(np.nanmean(right_scores)) if right_scores else 0.0]\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"Mean comparable score (higher is better)\")\n",
    "    plt.title(f\"Hemisphere summary — target: {target}\")\n",
    "    plt.tight_layout()\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(outdir / f\"hemisphere_summary_{target}.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Reporting ----------------\n",
    "\n",
    "def write_report(summaries: List[Dict[str,Any]], outdir: Path) -> Path:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    write_csv_rows(outdir / \"experiments_summary.csv\", summaries)\n",
    "\n",
    "    targets = sorted(set(s[\"target\"] for s in summaries))\n",
    "    for t in targets:\n",
    "        barplot_scores_by_family(summaries, t, outdir)\n",
    "        barplot_top_rois(summaries, t, outdir)\n",
    "        hemisphere_balance_plot(summaries, t, outdir)\n",
    "\n",
    "    md = []\n",
    "    md.append(\"# Cross-analysis Recap\\n\")\n",
    "    md.append(\"This report compares **periodic (delta/theta)**, **aperiodic (exponent/offset)** and **complexity (lziv/higuci)** families across targets (Barthel, FIM, Effectiveness).\")\n",
    "    md.append(\"Scores are transformed so that **higher is better** (e.g., RMSE/MAE are negated).\\n\")\n",
    "\n",
    "    for t in targets:\n",
    "        rows_t = [s for s in summaries if s[\"target\"]==t and np.isfinite(s[\"score_for_compare\"])]\n",
    "        md.append(f\"## Target: {t}\\n\")\n",
    "        if not rows_t:\n",
    "            md.append(\"_No valid experiments found._\\n\")\n",
    "            continue\n",
    "        by_fam = {}\n",
    "        for r in rows_t:\n",
    "            fam = r[\"family\"]\n",
    "            if fam not in by_fam or r[\"score_for_compare\"] > by_fam[fam][\"score_for_compare\"]:\n",
    "                by_fam[fam] = r\n",
    "        md.append(\"**Best by family**:\\n\")\n",
    "        for fam, r in by_fam.items():\n",
    "            md.append(f\"- {fam}: `{r['metric_name']}={r['metric_value']:.4f}` @ ROI `{r['roi_guess'] or r['feature_key']}`  (dir={r['metric_direction']})\")\n",
    "        md.append(\"\")\n",
    "\n",
    "        best_by_roi = {}\n",
    "        for r in rows_t:\n",
    "            roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "            if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "                best_by_roi[roi] = r\n",
    "        top5 = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:5]\n",
    "        md.append(\"**Top ROIs overall (best across families)**:\\n\")\n",
    "        for i,(roi, rr) in enumerate(top5, 1):\n",
    "            md.append(f\"{i}. `{roi}` — {rr['metric_name']}={rr['metric_value']:.4f} ({rr['family']})\")\n",
    "        md.append(\"\")\n",
    "\n",
    "        md.append(f\"![Best by family — {t}](best_by_family_{t}.png)\\n\")\n",
    "        md.append(f\"![Top ROIs — {t}](top_rois_{t}.png)\\n\")\n",
    "        md.append(f\"![Hemisphere summary — {t}](hemisphere_summary_{t}.png)\\n\")\n",
    "\n",
    "    report_path = outdir / \"cross_analysis_recap.md\"\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(md))\n",
    "    return report_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_root = Path(\"/Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\")\n",
    "outdir = results_root / \"_recap\"\n",
    "\n",
    "print(\"Results root:\", results_root)\n",
    "print(\"Output dir  :\", outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee48dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "results_root = Path(\"/Users/Altair93/Documents/Dottorato/PATHS/Python_analisys/Results/ML/results/univ_ROI\")  # <-- conferma questo path\n",
    "\n",
    "EXPECTED_ANY = {\n",
    "    \"metric_scores.csv\",\n",
    "    \"fold_scores.csv\",\n",
    "    \"feature_importances_mean.csv\",\n",
    "    \"predictions.csv\",\n",
    "    \"fold_predictions_long.csv\",\n",
    "    # i tuoi nomi reali:\n",
    "    \"metrics_by_fold.csv\",\n",
    "    \"feature_importances_mean_std.csv\",\n",
    "    \"feature_importances_by_fold.csv\",\n",
    "}\n",
    "\n",
    "csvs = list(results_root.rglob(\"*.csv\"))\n",
    "print(f\"CSV totali trovati: {len(csvs)}\")\n",
    "for p in csvs[:15]:\n",
    "    print(\"-\", p)\n",
    "\n",
    "# Cartelle candidate esperimento: contengono almeno UN file tra quelli attesi\n",
    "experiments = set()\n",
    "for p in csvs:\n",
    "    name = p.name.lower()\n",
    "    if (name in {n.lower() for n in EXPECTED_ANY}) or (name.startswith(\"best_fold_\") and name.endswith(\"_feature_importances.csv\")):\n",
    "        experiments.add(p.parent)\n",
    "\n",
    "experiments = sorted(experiments)\n",
    "print(f\"\\nCartelle esperimento trovate: {len(experiments)}\")\n",
    "for d in experiments[:20]:\n",
    "    print(\"-\", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23928e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = scan_results_root(results_root)\n",
    "print(f\"Found {len(summaries)} experiment folders with CSVs.\")\n",
    "\n",
    "report_path = write_report(summaries, outdir)\n",
    "print(\"Report written to:\", report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = (outdir / \"cross_analysis_recap.md\")\n",
    "if rep.exists():\n",
    "    with open(rep, \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"Report not found yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec39ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "outdir = results_root / \"_recap\"\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def read_csv_rows(path: Path):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "def safe_float(x, default=np.nan):\n",
    "    try: return float(x)\n",
    "    except: return default\n",
    "\n",
    "def infer_target_from_path(p: Path) -> str:\n",
    "    s = p.as_posix().lower()\n",
    "    if \"barthel\" in s: return \"barthel\"\n",
    "    if \"fim\" in s: return \"fim\"\n",
    "    if \"effectiveness\" in s: return \"effectiveness\"\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_family_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    if any(k in s for k in [\"delta\",\"theta\"]): return \"periodic\"\n",
    "    if any(k in s for k in [\"exponent\",\"offset\"]): return \"aperiodic\"\n",
    "    if any(k in s for k in [\"lziv\",\"higuci\",\"higuchi\"]): return \"complexity\"\n",
    "    return \"other\"\n",
    "\n",
    "def infer_feature_key_from_path(p: Path) -> str:\n",
    "    s = p.name.lower()\n",
    "    for k in [\"delta\",\"theta\",\"exponent\",\"offset\",\"lziv\",\"higuci\",\"higuchi\"]:\n",
    "        if k in s: return k\n",
    "    return \"unknown\"\n",
    "\n",
    "def infer_roi_from_importances(imp_rows):\n",
    "    if not imp_rows: return \"\"\n",
    "    feats = [r.get(\"feature\",\"\") for r in imp_rows if r.get(\"feature\")]\n",
    "    text = \"_\".join(feats).lower()\n",
    "    rois = [\"frontal\",\"temporal\",\"parietal\",\"occipital\",\"central\",\"cingulate\",\"hippocampus\",\"insula\"]\n",
    "    sides = [\"left\",\"right\",\"l\",\"r\"]\n",
    "    found_roi = next((roi for roi in rois if roi in text), None)\n",
    "    found_side = None\n",
    "    for s in sides:\n",
    "        if re.search(rf\"(^|[_\\-]){s}([_\\-]|$)\", text):\n",
    "            found_side = s; break\n",
    "    if found_roi and found_side: return f\"{found_roi}_{found_side}\"\n",
    "    return found_roi or \"\"\n",
    "\n",
    "def choose_score_for_comparison(metric_name, value, direction):\n",
    "    if value is None or np.isnan(value): return np.nan\n",
    "    if direction == \"lower_is_better\": return -value\n",
    "    return value\n",
    "\n",
    "def coerce_metric_rows_from_metrics_by_fold(exp_dir: Path):\n",
    "    rows = read_csv_rows(exp_dir / \"metrics_by_fold.csv\")\n",
    "    if not rows: return []\n",
    "    # cerca colonne comuni (case-insensitive)\n",
    "    keys = {k.lower(): k for k in rows[0].keys()}\n",
    "    candidates = [\"r2\",\"rmse\",\"mae\",\"mse\",\"accuracy\",\"f1\",\"mape\"]\n",
    "    out = []\n",
    "    for m in candidates:\n",
    "        k = keys.get(m)\n",
    "        if not k: continue\n",
    "        vals = [safe_float(r.get(k)) for r in rows]\n",
    "        vals = [v for v in vals if np.isfinite(v)]\n",
    "        if vals:\n",
    "            out.append({\"metric\": m, \"value\": float(np.mean(vals))})\n",
    "    return out\n",
    "\n",
    "def coerce_imp_rows_from_mean_std(exp_dir: Path):\n",
    "    rows = read_csv_rows(exp_dir / \"feature_importances_mean_std.csv\")\n",
    "    if not rows: return []\n",
    "    keys = {k.lower(): k for k in rows[0].keys()}\n",
    "    c_feat = keys.get(\"feature\") or keys.get(\"features\") or keys.get(\"name\")\n",
    "    c_mean = keys.get(\"mean_importance\") or keys.get(\"mean\") or keys.get(\"avg\")\n",
    "    c_std  = keys.get(\"std_importance\")  or keys.get(\"std\")  or keys.get(\"sd\")\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        feat = r.get(c_feat, \"\") if c_feat else \"\"\n",
    "        mi = safe_float(r.get(c_mean)) if c_mean else np.nan\n",
    "        sd = safe_float(r.get(c_std)) if c_std else np.nan\n",
    "        if feat:\n",
    "            out.append({\"feature\": feat, \"mean_importance\": mi, \"std_importance\": sd})\n",
    "    return out\n",
    "\n",
    "def best_metric_from_rows(metric_rows):\n",
    "    # priorità: R2 -> RMSE -> MAE -> Accuracy -> F1\n",
    "    m = { (r.get(\"metric\") or \"\").lower(): safe_float(r.get(\"value\")) for r in metric_rows }\n",
    "    if \"r2\" in m:      return (\"r2\", m[\"r2\"], \"higher_is_better\")\n",
    "    if \"rmse\" in m:    return (\"rmse\", m[\"rmse\"], \"lower_is_better\")\n",
    "    if \"mae\" in m:     return (\"mae\", m[\"mae\"], \"lower_is_better\")\n",
    "    if \"accuracy\" in m:return (\"accuracy\", m[\"accuracy\"], \"higher_is_better\")\n",
    "    if \"f1\" in m:      return (\"f1\", m[\"f1\"], \"higher_is_better\")\n",
    "    return (\"\", np.nan, \"higher_is_better\")\n",
    "\n",
    "# 1) individua esperimenti (tollerante, usa i tuoi nomi)\n",
    "EXPECTED_ANY = {\"metrics_by_fold.csv\",\"feature_importances_mean_std.csv\",\"feature_importances_by_fold.csv\",\n",
    "                \"metric_scores.csv\",\"fold_scores.csv\",\"feature_importances_mean.csv\",\"predictions.csv\",\"fold_predictions_long.csv\"}\n",
    "\n",
    "experiments = sorted({\n",
    "    p.parent for p in results_root.rglob(\"*.csv\")\n",
    "    if (p.name.lower() in {n.lower() for n in EXPECTED_ANY}) or (p.name.lower().startswith(\"best_fold_\") and p.name.lower().endswith(\"_feature_importances.csv\"))\n",
    "})\n",
    "\n",
    "print(\"Esperimenti trovati:\", len(experiments))\n",
    "for d in experiments[:10]:\n",
    "    print(\"-\", d)\n",
    "\n",
    "# 2) costruisci il summary per ciascun esperimento\n",
    "summaries = []\n",
    "for exp in experiments:\n",
    "    metric_rows = coerce_metric_rows_from_metrics_by_fold(exp)\n",
    "    imp_rows = coerce_imp_rows_from_mean_std(exp)\n",
    "\n",
    "    target = infer_target_from_path(exp)\n",
    "    family = infer_family_from_path(exp)\n",
    "    feature_key = infer_feature_key_from_path(exp)\n",
    "    roi_guess = infer_roi_from_importances(imp_rows)\n",
    "\n",
    "    metric_name, metric_value, direction = best_metric_from_rows(metric_rows)\n",
    "    score_for_compare = choose_score_for_comparison(metric_name, metric_value, direction) if metric_name else np.nan\n",
    "\n",
    "    # Top feature\n",
    "    top_feature = \"\"; top_mean_imp = np.nan; top_std_imp = np.nan\n",
    "    if imp_rows:\n",
    "        imp_sorted = sorted(imp_rows, key=lambda r: r[\"mean_importance\"] if np.isfinite(r[\"mean_importance\"]) else -1, reverse=True)\n",
    "        if imp_sorted:\n",
    "            top_feature = imp_sorted[0][\"feature\"]\n",
    "            top_mean_imp = imp_sorted[0][\"mean_importance\"]\n",
    "            top_std_imp = imp_sorted[0][\"std_importance\"]\n",
    "\n",
    "    summaries.append({\n",
    "        \"exp_dir\": exp.as_posix(),\n",
    "        \"target\": target,\n",
    "        \"family\": family,\n",
    "        \"feature_key\": feature_key,\n",
    "        \"roi_guess\": roi_guess or \"\",\n",
    "        \"metric_name\": metric_name,\n",
    "        \"metric_value\": metric_value,\n",
    "        \"metric_direction\": direction,\n",
    "        \"score_for_compare\": score_for_compare,\n",
    "        \"top_feature\": top_feature,\n",
    "        \"top_feature_mean_imp\": top_mean_imp,\n",
    "        \"top_feature_std_imp\": top_std_imp,\n",
    "    })\n",
    "\n",
    "# 3) salva tabella\n",
    "def write_csv_rows(path: Path, rows, fieldnames=None):\n",
    "    if not rows: return\n",
    "    if fieldnames is None:\n",
    "        fieldnames = list(rows[0].keys())\n",
    "        extra = set().union(*[set(r.keys()) for r in rows]) - set(fieldnames)\n",
    "        fieldnames += sorted(extra)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "write_csv_rows(outdir / \"experiments_summary.csv\", summaries)\n",
    "print(\"Salvato:\", outdir / \"experiments_summary.csv\")\n",
    "\n",
    "# 4) grafici per target\n",
    "def plot_best_by_family(summaries, target, outdir):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows: \n",
    "        print(\"[skip] best_by_family\", target); \n",
    "        return\n",
    "    best = {}\n",
    "    for r in rows:\n",
    "        fam = r[\"family\"]\n",
    "        if fam not in best or r[\"score_for_compare\"] > best[fam][\"score_for_compare\"]:\n",
    "            best[fam] = r\n",
    "    labels = list(best.keys())\n",
    "    vals = [best[k][\"score_for_compare\"] for k in labels]\n",
    "    plt.figure(figsize=(7,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Best by family — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"best_by_family_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_rois(summaries, target, outdir, top_k=12):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows: \n",
    "        print(\"[skip] top_rois\", target); \n",
    "        return\n",
    "    best_by_roi = {}\n",
    "    for r in rows:\n",
    "        roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "        if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "            best_by_roi[roi] = r\n",
    "    items = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:top_k]\n",
    "    labels = [k for k,_ in items]\n",
    "    vals = [v[\"score_for_compare\"] for _, v in items]\n",
    "    plt.figure(figsize=(10, max(4, 0.5*len(labels))))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Comparable score (higher is better)\")\n",
    "    plt.title(f\"Top ROIs — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"top_rois_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def hemisphere_from_label(label: str):\n",
    "    s = (label or \"\").lower().split(\"_\")\n",
    "    if \"right\" in s or \"r\" in s: return \"right\"\n",
    "    if \"left\" in s or \"l\" in s: return \"left\"\n",
    "    return None\n",
    "\n",
    "def plot_hemisphere_summary(summaries, target, outdir):\n",
    "    rows = [s for s in summaries if s[\"target\"]==target and np.isfinite(s[\"score_for_compare\"])]\n",
    "    if not rows:\n",
    "        print(\"[skip] hemisphere\", target)\n",
    "        return\n",
    "    left = []; right = []\n",
    "    for r in rows:\n",
    "        hemi = hemisphere_from_label(r.get(\"roi_guess\",\"\"))\n",
    "        if hemi == \"left\": left.append(r[\"score_for_compare\"])\n",
    "        elif hemi == \"right\": right.append(r[\"score_for_compare\"])\n",
    "    labels = [\"left\",\"right\"]\n",
    "    vals = [float(np.nanmean(left)) if left else 0.0,\n",
    "            float(np.nanmean(right)) if right else 0.0]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    x = np.arange(len(labels))\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"Mean comparable score\")\n",
    "    plt.title(f\"Hemisphere summary — {target}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outdir / f\"hemisphere_summary_{target}.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "targets = sorted(set(s[\"target\"] for s in summaries))\n",
    "for t in targets:\n",
    "    plot_best_by_family(summaries, t, outdir)\n",
    "    plot_top_rois(summaries, t, outdir)\n",
    "    plot_hemisphere_summary(summaries, t, outdir)\n",
    "\n",
    "# 5) report markdown\n",
    "md = []\n",
    "md.append(\"# Cross-analysis Recap\\n\")\n",
    "md.append(\"Confronto tra **periodic (delta/theta)**, **aperiodic (exponent/offset)** e **complexity (lziv/higuci)** sui target (Barthel, FIM, Effectiveness).\")\n",
    "md.append(\"Gli score sono resi comparabili come *higher is better* (MAE/RMSE negati).\\\\n\")\n",
    "\n",
    "for t in targets:\n",
    "    rows_t = [s for s in summaries if s[\"target\"]==t and np.isfinite(s[\"score_for_compare\"])]\n",
    "    md.append(f\"## Target: {t}\\\\n\")\n",
    "    if not rows_t:\n",
    "        md.append(\"_Nessun esperimento valido._\\\\n\")\n",
    "        continue\n",
    "    by_fam = {}\n",
    "    for r in rows_t:\n",
    "        fam = r[\"family\"]\n",
    "        if fam not in by_fam or r[\"score_for_compare\"] > by_fam[fam][\"score_for_compare\"]:\n",
    "            by_fam[fam] = r\n",
    "    md.append(\"**Best by family**:\\\\n\")\n",
    "    for fam, r in by_fam.items():\n",
    "        md.append(f\"- {fam}: `{r['metric_name']}={r['metric_value']:.4f}` @ ROI `{r['roi_guess'] or r['feature_key']}` (dir={r['metric_direction']})\")\n",
    "    md.append(\"\")\n",
    "    best_by_roi = {}\n",
    "    for r in rows_t:\n",
    "        roi = r[\"roi_guess\"] or r[\"feature_key\"]\n",
    "        if roi not in best_by_roi or r[\"score_for_compare\"] > best_by_roi[roi][\"score_for_compare\"]:\n",
    "            best_by_roi[roi] = r\n",
    "    top5 = sorted(best_by_roi.items(), key=lambda kv: kv[1][\"score_for_compare\"], reverse=True)[:5]\n",
    "    md.append(\"**Top ROIs overall**:\\\\n\")\n",
    "    for i,(roi, rr) in enumerate(top5, 1):\n",
    "        md.append(f\"{i}. `{roi}` — {rr['metric_name']}={rr['metric_value']:.4f} ({rr['family']})\")\n",
    "    md.append(\"\")\n",
    "    md.append(f\"![Best by family — {t}](best_by_family_{t}.png)\\\\n\")\n",
    "    md.append(f\"![Top ROIs — {t}](top_rois_{t}.png)\\\\n\")\n",
    "    md.append(f\"![Hemisphere summary — {t}](hemisphere_summary_{t}.png)\\\\n\")\n",
    "\n",
    "report_path = outdir / \"cross_analysis_recap.md\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(md))\n",
    "\n",
    "print(\"Report scritto in:\", report_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
